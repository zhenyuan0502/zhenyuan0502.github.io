
<!DOCTYPE html>
<html lang="en" class="loading">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>LLM-Powered Localization Pipeline #3: Prompt Engineering and Validation - Nguyen Luu Blog</title>
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="google" content="notranslate" />
    <meta name="keywords" content="Nguyen Luu zhenyuan0502,"> 
    <meta name="description" content="In the previous installments of this series, we established the foundation for an LLM-powered local,"> 
    <meta name="author" content="Nguyen Luu"> 
    <link rel="alternative" href="atom.xml" title="Nguyen Luu Blog" type="application/atom+xml"> 
    <link rel="icon" href="/img/favicon.png"> 
    
<link rel="stylesheet" href="//at.alicdn.com/t/font_1429596_nzgqgvnmkjb.css">

    
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/animate.css/4.1.1/animate.min.css">

    
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/css/share.min.css">

    
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/codemirror/5.48.4/codemirror.min.css">

    
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/codemirror/5.48.4/theme/dracula.css">

    
<link rel="stylesheet" href="/css/obsidian.css">

    

    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="//www.googletagmanager.com/gtag/js?id=UA-193142270-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag('js', new Date());

      gtag('config', 'UA-193142270-1');
    </script>
    

    
<meta name="generator" content="Hexo 7.3.0"></head>


<body class="loading">
    <span id="config-title" style="display:none">Nguyen Luu Blog</span>
    <div id="single">
    <div class="scrollbar gradient-bg-rev"></div>
<div id="top" style="display: block;">
    <div class="bar" style="width: 0;"></div>
    <div class="navigation">
        <img id="home-icon" class="icon-home" src="/img/favicon.png" alt="" data-url="/">
        <!-- <div id="play-icon" title="Play/Pause" class="iconfont icon-play"></div> -->
        <h3 class="home" alt="" data-url="/">Home</h3>
        <h3 class="subtitle">LLM-Powered Localization Pipeline #3: Prompt Engineering and Validation</h3>
        <div class="social">
            <!--        <div class="like-icon">-->
            <!--            <a href="javascript:;" class="likeThis active"><span class="icon-like"></span><span class="count">76</span></a>-->
            <!--        </div>-->
            <div>
                <div class="share">
                    
                </div>
            </div>
        </div>
    </div>
</div>

    <div class="section">
        <div class=article-header-wrapper>
    <div class="article-header">
        <!-- <div class="article-cover animated fadeIn" style="
            animation-delay: 600ms;
            animation-duration: 1.2s;
            background-image: 
                radial-gradient(ellipse closest-side, rgba(0, 0, 0, 0.65), #100e17),
                url(/null) ">
        </div> -->
        <div class="else">
            <p class="animated fadeInDown">
                
                <a href="/categories/LLM"><b>「
                    </b>LLM<b> 」</b></a>
                
                June 06, 2025
            </p>
            <h3 class="post-title animated fadeInDown"><a href="/post/2025/06/06/LLM-Powered-Localization-Pipeline-3-Prompt-Engineering-and-Validation" title="LLM-Powered Localization Pipeline #3: Prompt Engineering and Validation" class="">LLM-Powered Localization Pipeline #3: Prompt Engineering and Validation</a>
            </h3>
            
            <p class="post-count animated fadeInDown">
                
                <span>
                    <b class="iconfont icon-text2"></b> <i>Words count</i>
                    28k
                </span>
                
                
                <span>
                    <b class="iconfont icon-timer__s"></b> <i>Reading time</i>
                    26 mins.
                </span>
                
                
                
            </p>
            
            
            <ul class="animated fadeInDown post-tags-list" itemprop="keywords"><li class="animated fadeInDown post-tags-list-item"><a class="animated fadeInDown post-tags-list-link" href="/tags/Automation/" rel="tag">Automation</a></li><li class="animated fadeInDown post-tags-list-item"><a class="animated fadeInDown post-tags-list-link" href="/tags/LLM/" rel="tag">LLM</a></li><li class="animated fadeInDown post-tags-list-item"><a class="animated fadeInDown post-tags-list-link" href="/tags/Localization/" rel="tag">Localization</a></li><li class="animated fadeInDown post-tags-list-item"><a class="animated fadeInDown post-tags-list-link" href="/tags/TMS/" rel="tag">TMS</a></li><li class="animated fadeInDown post-tags-list-item"><a class="animated fadeInDown post-tags-list-link" href="/tags/Translation/" rel="tag">Translation</a></li><li class="animated fadeInDown post-tags-list-item"><a class="animated fadeInDown post-tags-list-link" href="/tags/Translation-Management-System/" rel="tag">Translation Management System</a></li><li class="animated fadeInDown post-tags-list-item"><a class="animated fadeInDown post-tags-list-link" href="/tags/Translation-Quality/" rel="tag">Translation Quality</a></li><li class="animated fadeInDown post-tags-list-item"><a class="animated fadeInDown post-tags-list-link" href="/tags/Validation/" rel="tag">Validation</a></li></ul>
            
        </div>
    </div>
</div>

<div class="screen-gradient-after">
    <div class="screen-gradient-content">
        <div class="screen-gradient-content-inside">
            <div class="bold-underline-links screen-gradient-sponsor">
                <p>
                    <span class="animated fadeIn delay-1s"></span>
                </p>
            </div>
        </div>
    </div>
</div>

<div class="article">
    <div class='main'>
        <div class="content markdown animated fadeIn">
            <p>In the previous installments of this series, we established the foundation for an LLM-powered localization pipeline. In Part 2, we structured our approach further by transforming dictionary data, implementing chunking strategies, and creating translation guides to ensure consistency. We will dive into the critical elements of prompt engineering and validation – the components that determine how effectively our LLMs translate content and how we verify the quality of their output.</p>
<hr>
<h1 id="The-Art-of-LLM-Prompt-Engineering-for-Translation"><a href="#The-Art-of-LLM-Prompt-Engineering-for-Translation" class="headerlink" title="The Art of LLM Prompt Engineering for Translation"></a>The Art of LLM Prompt Engineering for Translation</h1><p>Let’s take a look at again the localization pipeline we are going to build, we are in step 9:</p>
<p><img src="/images/llm_localization_pipeline.png"></p>
<h2 id="9-LLM-Prompt-Design-and-Processing"><a href="#9-LLM-Prompt-Design-and-Processing" class="headerlink" title="9. LLM Prompt Design and Processing"></a>9. LLM Prompt Design and Processing</h2><p>Effective prompt engineering is the cornerstone of achieving high-quality translations with LLMs. Let’s examine how to structure prompts that maximize translation accuracy and adherence to guidelines.</p>
<h3 id="The-Anatomy-of-an-Effective-Translation-Prompt"><a href="#The-Anatomy-of-an-Effective-Translation-Prompt" class="headerlink" title="The Anatomy of an Effective Translation Prompt"></a>The Anatomy of an Effective Translation Prompt</h3><p>A well-designed translation prompt should include:</p>
<ul>
<li><strong>Clear instructions</strong> about the translation task</li>
<li><strong>Context</strong> for the content being translated</li>
<li><strong>Reference to the translation guide</strong> and its rules</li>
<li><strong>Examples</strong> of correct translations when helpful</li>
<li><strong>Dictionary references</strong> for key terminology</li>
<li><strong>Expected output format</strong> for consistency</li>
</ul>
<p>Before we implement the prompt request, we need to define pydantic model for the response format:</p>
<pre><code class="highlight python"><span class="keyword">from</span> pydantic <span class="keyword">import</span> BaseModel
<span class="keyword">class</span> <span class="title class_">MXIFF_Target</span>(<span class="title class_ inherited__">BaseModel</span>):
    trans_unit_id: <span class="built_in">str</span>
    target: <span class="built_in">str</span>
    
<span class="keyword">class</span> <span class="title class_">MXIFF_Response</span>(<span class="title class_ inherited__">BaseModel</span>):
    data: <span class="built_in">list</span>[MXIFF_Target]</code></pre>


<p>Here’s an example of how we might implement the <code>post_to_llm()</code> function mentioned in the previous post:</p>
<pre><code class="highlight python">
<span class="keyword">from</span> openai <span class="keyword">import</span> OpenAI, AzureOpenAI
<span class="keyword">import</span> json

client = AzureOpenAI(
    azure_endpoint = endpoint,
    api_key = api_key,
    api_version= api_version
)

<span class="keyword">def</span> <span class="title function_">post_to_llm</span>(<span class="params">chunk_dict, translation_guide, focus_prompt=<span class="string">&quot;&quot;</span>, system_prompt=<span class="literal">None</span>, model=<span class="string">&quot;gpt-4o&quot;</span>, number_of_tries=<span class="number">0</span></span>):
    <span class="string">&quot;&quot;&quot;</span>
<span class="string">    Send a chunk of translation items to the LLM for processing.</span>
<span class="string">    </span>
<span class="string">    Args:</span>
<span class="string">        chunk_dict: Dictionary of items to translate</span>
<span class="string">        translation_guide: Guidelines for translation</span>
<span class="string">        focus_prompt: Additional instructions or feedback from previous attempts</span>
<span class="string">        system_prompt: System-level instructions for the LLM</span>
<span class="string">        model: The LLM model to use</span>
<span class="string">        number_of_tries: Number of previous attempts (affects temperature setting)</span>
<span class="string">    </span>
<span class="string">    Returns:</span>
<span class="string">        List of translated items with their trans_unit_ids and target texts</span>
<span class="string">    &quot;&quot;&quot;</span>
    <span class="keyword">if</span> system_prompt <span class="keyword">is</span> <span class="literal">None</span>:
        system_prompt = <span class="string">&quot;&quot;&quot;</span>
<span class="string">        You are a professional translator specializing in accurate, context-aware translations.</span>
<span class="string">        Follow the translation guide precisely and maintain all formatting elements.</span>
<span class="string">        Generate JSON output with translated text that preserves all placeholders and special characters.</span>
<span class="string">        &quot;&quot;&quot;</span>
    
    <span class="comment"># Convert chunk_dict to list format for the prompt</span>
    items_to_translate = <span class="built_in">list</span>(chunk_dict.values())
    
    <span class="comment"># Build the user prompt</span>
    user_prompt = <span class="string">f&quot;&quot;&quot;</span>
<span class="string">    # Translation Task</span>
<span class="string">    </span>
<span class="string">    Translate the following items from Chinese to English according to the translation guide below.</span>
<span class="string">    </span>
<span class="string">    ## Translation Guide:</span>
<span class="string">    <span class="subst">&#123;translation_guide&#125;</span></span>
<span class="string">    </span>
<span class="string">    ## Special Instructions:</span>
<span class="string">    <span class="subst">&#123;focus_prompt <span class="keyword">if</span> focus_prompt <span class="keyword">else</span> <span class="string">&quot;No special instructions for this batch.&quot;</span>&#125;</span></span>
<span class="string">    </span>
<span class="string">    ## Items to Translate:</span>
<span class="string">    <span class="subst">&#123;json.dumps(items_to_translate, indent=<span class="number">2</span>, ensure_ascii=<span class="literal">False</span>)&#125;</span></span>
<span class="string">    </span>
<span class="string">    ## Output Format:</span>
<span class="string">    Return ONLY a JSON array of objects, each with &#x27;trans_unit_id&#x27; and &#x27;target&#x27; fields.</span>
<span class="string">    Example output format:</span>
<span class="string">    [</span>
<span class="string">      &#123;&#123;</span>
<span class="string">        &quot;trans_unit_id&quot;: &quot;unit_123&quot;,</span>
<span class="string">        &quot;target&quot;: &quot;Translated text with <span class="subst">&#123;<span class="number">1</span>&#125;</span> preserved placeholders <span class="subst">&#123;<span class="number">2</span>&#125;</span>&quot;</span>
<span class="string">      &#125;&#125;</span>
<span class="string">    ]</span>
<span class="string">    &quot;&quot;&quot;</span>
    
    <span class="comment"># Calculate temperature - decreases with more attempts for more conservative outputs</span>
    temperature = <span class="number">0.2</span> ** (<span class="number">1</span>/(number_of_tries + <span class="number">1</span>))
    
    <span class="comment"># Make API call to the LLM with response validation through Pydantic</span>
    response = client.beta.chat.completions.parse(
        model=model,
        messages=[
            &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: system_prompt&#125;,
            &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: user_prompt&#125;
        ],
        temperature=temperature,
        response_format=&#123;<span class="string">&quot;type&quot;</span>: <span class="string">&quot;json_object&quot;</span>&#125;
    )
    
    <span class="comment"># Parse and validate the response using our Pydantic model</span>
    result_json = json.loads(response.choices[<span class="number">0</span>].message.content)
    
    <span class="keyword">return</span> result_json[<span class="string">&#x27;data&#x27;</span>]</code></pre>

<p>Why we increase the <code>temperature</code> with the number of attempts? The idea is to encourage the LLM to explore different translation options as it learns from previous mistakes. This can lead to more creative and potentially better translations over time. Whereas the <code>focus_prompt</code> allows us to provide specific feedback on what needs to be improved, such as correcting untranslated terms or adjusting formatting from previous attempts.</p>
<p>Finally the response look like this:</p>
<pre><code class="highlight json"><span class="punctuation">&#123;</span>
    <span class="attr">&quot;data&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span>
        <span class="punctuation">&#123;</span>
            <span class="attr">&quot;trans_unit_id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;unit_123&quot;</span><span class="punctuation">,</span>
            <span class="attr">&quot;target&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Translated text with &#123;1&#125; preserved placeholders &#123;2&#125;&quot;</span>
        <span class="punctuation">&#125;</span>
    <span class="punctuation">]</span>
<span class="punctuation">&#125;</span>
</code></pre>

<p>By structuring our prompts with clear instructions, context, and examples, we enable the LLM to generate accurate and consistent translations. The use of Pydantic models for response validation ensures we get properly formatted data that can be safely processed in subsequent pipeline stages.</p>
<p>At the time implementing this, we were facing some content filtering issue with Azure OpenAI, which alerts us few categories have been triggered, such as “Hate Speech”, “Violence”, etc. To avoid this, we can prompt the LLM to avoid generating such content. However, we may adjust blocking higer threashold or even disable the content filtering in the API call if we are confident that our content is safe and appropriate - by filling this form <a target="_blank" rel="noopener" href="https://ncv.microsoft.com/uEfCgnITdR">https://ncv.microsoft.com/uEfCgnITdR</a>, read more about it here <a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/content-filters">https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/content-filters</a></p>
<h3 id="Iterative-Refinement-Through-Feedback-Loops"><a href="#Iterative-Refinement-Through-Feedback-Loops" class="headerlink" title="Iterative Refinement Through Feedback Loops"></a>Iterative Refinement Through Feedback Loops</h3><p>A key advantage of our chunking approach is the ability to provide focused feedback when translations don’t meet our requirements. The <code>focus_prompt</code> parameter allows us to communicate specific issues to the LLM:</p>
<pre><code class="highlight python"><span class="comment"># Example of updating focus_prompt after validation</span>
<span class="keyword">if</span> <span class="keyword">not</span> validation_result[<span class="string">&#x27;is_valid&#x27;</span>]:
    focus_prompt += <span class="string">f&quot;\n- [<span class="subst">&#123;trans_unit_id&#125;</span>] When translating \&quot;<span class="subst">&#123;source&#125;</span>\&quot;, please note: <span class="subst">&#123;validation_result[<span class="string">&#x27;invalid_reason&#x27;</span>]&#125;</span>&quot;</span>
    focus_prompt += <span class="string">f&quot;\n- [<span class="subst">&#123;trans_unit_id&#125;</span>] Correct any Chinese terms that weren&#x27;t properly translated.&quot;</span></code></pre>

<p>This feedback mechanism creates a learning loop where the LLM improves its translations based on specific guidance.</p>
<h3 id="Model-Selection-and-Parameter-Tuning"><a href="#Model-Selection-and-Parameter-Tuning" class="headerlink" title="Model Selection and Parameter Tuning"></a>Model Selection and Parameter Tuning</h3><p>Different LLM models and parameter settings can significantly impact translation quality. Some considerations:</p>
<ul>
<li><strong>Temperature</strong>: Lower values (0.0-0.3) for more deterministic translations, higher values (0.7-1.0) for more creative options</li>
<li><strong>Top-p sampling</strong>: Affects diversity of word choices</li>
<li><strong>Model selection</strong>: Newer models often have better multilingual capabilities</li>
<li><strong>Context window</strong>: Larger models can handle more context at once</li>
</ul>
<pre><code class="highlight python"><span class="comment"># Example of model and parameter selection</span>

response = client.beta.chat.completions.parse(
    model=<span class="string">&quot;gpt-4o&quot;</span>,  <span class="comment"># Or gpt-4-turbo, Claude 3, etc.</span>
    messages=[...],
    temperature=<span class="number">0.2</span>,  <span class="comment"># Low temperature for consistency</span>
    top_p=<span class="number">0.95</span>,
    ...
)</code></pre>

<h2 id="10-Validation-and-Quality-Assurance"><a href="#10-Validation-and-Quality-Assurance" class="headerlink" title="10. Validation and Quality Assurance"></a>10. Validation and Quality Assurance</h2><p>Before going to validation, we may implement few plugins validation, like:</p>
<ul>
<li>Normalize the <code>target</code> text to NFC (Normalization Form C) to ensure consistent character representation</li>
</ul>
<pre><code class="highlight python"><span class="keyword">import</span> unicodedata
<span class="keyword">def</span> <span class="title function_">normalize_text</span>(<span class="params">text</span>):
    <span class="comment"># Normalize to NFC (composed form)</span>
    <span class="keyword">return</span> unicodedata.normalize(<span class="string">&#x27;NFC&#x27;</span>, text).strip()</code></pre>

<ul>
<li>Detect chinese characters in the <code>target</code></li>
</ul>
<pre><code class="highlight python"><span class="keyword">import</span> re

CHINESE_WORD_PATTERN = re.<span class="built_in">compile</span>(<span class="string">r&#x27;[\u3400-\u4dbf\u4e00-\u9fff\uf900-\ufaff]+&#x27;</span>)
<span class="keyword">def</span> <span class="title function_">detect_chinese_with_regex</span>(<span class="params">text</span>):
    <span class="comment"># Define a regex pattern to match Chinese characters</span>
    <span class="comment"># Find all Chinese words in the text</span>
    chinese_words = CHINESE_WORD_PATTERN.findall(text)
    <span class="keyword">return</span> chinese_words</code></pre>

<ul>
<li>Check placeholders in the <code>target</code> against the <code>source</code> text to ensure they are preserved correctly. This is especially important for UI strings or content with dynamic elements. <code>is_variable_placeholders_equal(source, target, tunit_dict)</code> compares the variable placeholders in the source and target texts, using metadata from <code>tunit_dict</code>. It returns whether the number and order of placeholders match between the source and target, along with the lists of detected placeholders from each. <code>get_variable_placeholders(text)</code> extracts all variable placeholders enclosed in curly braces <code>&#123;&#125;</code> from the given text and returns them as a list, preserving their order of appearance.</li>
</ul>
<pre><code class="highlight python"><span class="keyword">def</span> <span class="title function_">is_variable_placeholders_equal</span>(<span class="params">source, target, tunit_dict</span>):
    <span class="keyword">if</span> tunit_dict <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> <span class="built_in">len</span>(tunit_dict) == <span class="number">0</span>:
        <span class="keyword">return</span> <span class="literal">True</span>, [], []
        
    target_variables = get_variable_placeholders(target)
    source_variables = get_variable_placeholders(source)
    
    is_valid = (<span class="built_in">len</span>(target_variables) == <span class="built_in">len</span>(source_variables) == <span class="built_in">len</span>(tunit_dict))
    <span class="keyword">return</span> is_valid, source_variables, target_variables
    

<span class="keyword">def</span> <span class="title function_">get_variable_placeholders</span>(<span class="params">text</span>):
    queue = []
    index = <span class="number">0</span> 
    <span class="keyword">while</span> index &lt; <span class="built_in">len</span>(text):
        <span class="keyword">if</span> text[index] == <span class="string">&#x27;&#123;&#x27;</span>:
            var_start = index
            <span class="comment"># find next &#x27;&#125;&#x27;</span>
            <span class="keyword">while</span> index &lt; <span class="built_in">len</span>(text) <span class="keyword">and</span> text[index] != <span class="string">&#x27;&#125;&#x27;</span>:
                index += <span class="number">1</span>
                
            index += <span class="number">1</span>
            queue.append(text[var_start+<span class="number">1</span>:index-<span class="number">1</span>])
        <span class="keyword">else</span>:
            index += <span class="number">1</span>
    
    <span class="keyword">return</span> queue</code></pre>

<p>You can add more plugins as needed in your specific use case, as it is crucial to ensure translations meet quality standards. Let’s implement a comprehensive validation function:</p>
<pre><code class="highlight python"><span class="keyword">def</span> <span class="title function_">validate</span>(<span class="params">translation_item: MLXIFF_Target, item_data_level_2: <span class="built_in">dict</span></span>) -&gt; <span class="built_in">dict</span>:
    trans_unit_id = translation_item.get(<span class="string">&#x27;trans_unit_id&#x27;</span>, <span class="string">&#x27;&#x27;</span>)

    <span class="keyword">if</span> item_data_level_2 <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(item_data_level_2, <span class="built_in">dict</span>):
        <span class="keyword">return</span> &#123;
            <span class="string">&#x27;is_valid&#x27;</span>: <span class="literal">False</span>,
            <span class="string">&#x27;invalid_reason&#x27;</span>: <span class="string">f&quot;Does not match <span class="subst">&#123;trans_unit_id&#125;</span> in item_data_level_2.&quot;</span>
        &#125;

    tunit_metadata = item_data_level_2.get(<span class="string">&#x27;tunit_metadata&#x27;</span>, &#123;&#125;)
    target = translation_item.get(<span class="string">&#x27;target&#x27;</span>, <span class="string">&#x27;&#x27;</span>)
    source = item_data_level_2.get(<span class="string">&#x27;source&#x27;</span>, <span class="string">&#x27;&#x27;</span>)
    
    <span class="keyword">if</span> target <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> target.strip() == <span class="string">&#x27;&#x27;</span>:
        <span class="keyword">return</span> &#123;
            <span class="string">&#x27;is_valid&#x27;</span>: <span class="literal">False</span>,
            <span class="string">&#x27;invalid_reason&#x27;</span>: <span class="string">&quot;Missing translation.&quot;</span>
        &#125;

    target = normalize_text(target)
    
    <span class="comment"># Check for untranslated Chinese characters</span>
    chinese_words = detect_chinese_with_regex(target)
    <span class="keyword">if</span> chinese_words:
        <span class="keyword">return</span> &#123; 
            <span class="string">&#x27;is_valid&#x27;</span>: <span class="literal">False</span>,
            <span class="string">&#x27;invalid_reason&#x27;</span>: result[<span class="string">&#x27;invalid_reason&#x27;</span>]
        &#125;
    
    is_valid_placeholder, source_variables, target_variables = is_variable_placeholders_equal(source, target, tunit_metadata)
    <span class="keyword">if</span> <span class="keyword">not</span> is_valid_placeholder:
        <span class="keyword">return</span> &#123;
            <span class="string">&#x27;is_valid&#x27;</span>: <span class="literal">False</span>,
            <span class="string">&#x27;invalid_reason&#x27;</span>: <span class="string">f&quot;Insufficient variable detected <span class="subst">&#123;source&#125;</span> -&gt; <span class="subst">&#123;target&#125;</span> | <span class="subst">&#123;source_variables&#125;</span> -&gt; <span class="subst">&#123;target_variables&#125;</span> | <span class="subst">&#123;tunit_metadata&#125;</span>&quot;</span>
        &#125;
    
    <span class="keyword">return</span> &#123;
        <span class="string">&#x27;is_valid&#x27;</span>: <span class="literal">True</span>,
        <span class="string">&#x27;invalid_reason&#x27;</span>: <span class="string">&quot;&quot;</span>
    &#125;</code></pre>

<p>The function checks whether a translated text (target) meets quality requirements when compared to its source text. It performs several validation checks:</p>
<ul>
<li>Verifies that the metadata reference (item_data_level_2) exists and is properly formatted</li>
<li>Ensures the translation is not empty</li>
<li>Normalizes the translated text for consistent processing</li>
<li>Checks for untranslated Chinese characters that might have been accidentally left in the translation</li>
<li>Validates that variable placeholders (like {0} or format specifiers) are correctly preserved between the source and translated text</li>
<li>The function returns a dictionary with an is_valid boolean and an invalid_reason string explaining any failures.</li>
</ul>
<h3 id="Advanced-Validation-Techniques"><a href="#Advanced-Validation-Techniques" class="headerlink" title="Advanced Validation Techniques"></a>Advanced Validation Techniques</h3><p>For more sophisticated validation, consider:</p>
<ul>
<li><strong>Semantic equivalence checking</strong>: Use another LLM to verify that the meaning is preserved</li>
<li><strong>Terminology consistency</strong>: Ensure dictionary terms are used consistently</li>
<li><strong>Style guide adherence</strong>: Check if translations follow stylistic rules</li>
<li><strong>Back-translation</strong>: Translate back to the source language to check for major meaning shifts</li>
</ul>
<p>Here’s how you might implement a semantic validation check:</p>
<pre><code class="highlight python"><span class="keyword">def</span> <span class="title function_">validate_semantic_equivalence</span>(<span class="params">source, target, source_lang, target_lang</span>):
    <span class="string">&quot;&quot;&quot;Validate semantic equivalence between source and translation.&quot;&quot;&quot;</span>
    
    prompt = <span class="string">f&quot;&quot;&quot;</span>
<span class="string">    Evaluate if the translation preserves the meaning of the original text.</span>
<span class="string">    </span>
<span class="string">    Source (<span class="subst">&#123;source_lang&#125;</span>): <span class="subst">&#123;source&#125;</span></span>
<span class="string">    Translation (<span class="subst">&#123;target_lang&#125;</span>): <span class="subst">&#123;target&#125;</span></span>
<span class="string">    </span>
<span class="string">    Rate the semantic equivalence on a scale of 1-5 where:</span>
<span class="string">    1 = Major meaning loss or distortion</span>
<span class="string">    2 = Some important meaning lost</span>
<span class="string">    3 = Core meaning preserved but nuances lost</span>
<span class="string">    4 = Good preservation of meaning with minor differences</span>
<span class="string">    5 = Excellent equivalence in meaning</span>
<span class="string">    </span>
<span class="string">    Provide your rating and explanation in JSON format:</span>
<span class="string">    &#123;&#123;</span>
<span class="string">      &quot;rating&quot;: [1-5],</span>
<span class="string">      &quot;explanation&quot;: &quot;Your explanation here&quot;,</span>
<span class="string">      &quot;is_acceptable&quot;: [true/false]</span>
<span class="string">    &#125;&#125;</span>
<span class="string">    &quot;&quot;&quot;</span>
    
    response = client.chat.completions.create(
        model=<span class="string">&quot;gpt-4o&quot;</span>,
        messages=[
            &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;You are an expert translation validator.&quot;</span>&#125;,
            &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: prompt&#125;
        ],
        response_format=&#123;<span class="string">&quot;type&quot;</span>: <span class="string">&quot;json_object&quot;</span>&#125;
    )
    
    result = json.loads(response.choices[<span class="number">0</span>].message.content)
    <span class="keyword">return</span> &#123;
        <span class="string">&quot;is_valid&quot;</span>: result.get(<span class="string">&quot;is_acceptable&quot;</span>, <span class="literal">False</span>),
        <span class="string">&quot;invalid_reason&quot;</span>: result.get(<span class="string">&quot;explanation&quot;</span>, <span class="string">&quot;Semantic equivalence issue&quot;</span>) <span class="keyword">if</span> <span class="keyword">not</span> result.get(<span class="string">&quot;is_acceptable&quot;</span>, <span class="literal">False</span>) <span class="keyword">else</span> <span class="string">&quot;&quot;</span>
    &#125;</code></pre>

<p>Be note that I did not use pydantic model for the response here, instead, <code>&#123;&quot;type&quot;: &quot;json_object&quot;&#125;</code> is an another way to parse json.</p>
<h1 id="Wrapping-up"><a href="#Wrapping-up" class="headerlink" title="Wrapping up"></a>Wrapping up</h1><p>Now let’s put it all together. The overall flow integrates prompt engineering and validation in an iterative process:</p>
<ul>
<li><strong>Initial translation</strong>: Submit chunks to the LLM with clear instructions</li>
<li><strong>Validation</strong>: Check each translation against quality criteria</li>
<li><strong>Feedback loop</strong>: For invalid translations, provide specific feedback</li>
<li><strong>Re-translation</strong>: Submit problematic translations with enhanced instructions</li>
<li><strong>Hygiene</strong>: Run additional plugins to correct common issues but do not need to resubmit to the LLM</li>
</ul>
<p>In the next installment, we’ll explore how to hygienize and finalize translations, ensuring they are ready for delivery to clients or integration into applications. We’ll also discuss how to handle edge cases and maintain translation quality over time.</p>
<blockquote>
<p>Series LLM-Powered Localization Pipeline:</p>
<ul>
<li><a href="/post/2025/06/02/LLM-Powered-Localization-Pipeline-1-Faster-Smarter-and-More-Accurate-Translation" title="LLM-Powered Localization Pipeline #1: Faster, Smarter and More Accurate Translation">LLM-Powered Localization Pipeline #1: Faster, Smarter and More Accurate Translation</a></li>
<li><a href="/post/2025/06/02/LLM-Powered-Localization-Pipeline-2-When-Generative-AI-Follows-the-Rules" title="LLM-Powered Localization Pipeline #2: When Generative AI Follows the Rules">LLM-Powered Localization Pipeline #2: When Generative AI Follows the Rules</a></li>
<li><a href="/post/2025/06/06/LLM-Powered-Localization-Pipeline-3-Prompt-Engineering-and-Validation" title="LLM-Powered Localization Pipeline #3: Prompt Engineering and Validation">LLM-Powered Localization Pipeline #3: Prompt Engineering and Validation</a></li>
<li><a href="/post/2025/06/10/LLM-Powered-Localization-Pipeline-4-Hygiene-for-Translated-Records" title="LLM-Powered Localization Pipeline #4: Hygiene for Translated Records">LLM-Powered Localization Pipeline #4: Hygiene for Translated Records</a></li>
</ul>
</blockquote>

            <!--[if lt IE 9]><script>document.createElement('audio');</script><![endif]-->
            <!-- <audio id="audio" loop="1" preload="auto" controls="controls"
                data-autoplay="false">
                <source type="audio/mpeg" src="">
            </audio>
            
            <ul id="audio-list" style="display:none">
                
                
                <li title='0' data-url='/statics/shijiandeguoke.mp3'></li>
                
                    
            </ul>
             -->

            
            


<div class="post-copyright-container">
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Author</strong>
    Nguyen Luu
  </li>
  <li class="post-copyright-link">
    <strong>Link</strong>
    <a href="https://nguyenluu.dev/post/2025/06/06/LLM-Powered-Localization-Pipeline-3-Prompt-Engineering-and-Validation" title="LLM-Powered Localization Pipeline #3: Prompt Engineering and Validation">https://nguyenluu.dev/post/2025/06/06/LLM-Powered-Localization-Pipeline-3-Prompt-Engineering-and-Validation</a>
  </li>
  <li class="post-copyright-license">
    <strong>License</strong>
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>by-nc-sa</a>
  </li>
</ul>
</div>

            

            <!-- 
            <div class="post-nav">
                <hr>
                
                    <div class="post-nav-item">Previous Post: <a href="/post/2025/06/10/LLM-Powered-Localization-Pipeline-4-Hygiene-for-Translated-Records" rel="prev" 
                        title="LLM-Powered Localization Pipeline #4: Hygiene for Translated Records">LLM-Powered Localization Pipeline #4: Hygiene for Translated Records
                      </a></div>
                
                
                    <div class="post-nav-item">Next Post: <a href="/post/2025/06/02/LLM-Powered-Localization-Pipeline-2-When-Generative-AI-Follows-the-Rules" rel="next" 
                        title="LLM-Powered Localization Pipeline #2: When Generative AI Follows the Rules">LLM-Powered Localization Pipeline #2: When Generative AI Follows the Rules</a></div>
                                
            </div>   
             -->
            
            

            
            
    <div id='gitalk-container' class="comment link"
        data-ae='true'
        data-ci='Ov23li5wi821AxHmkjCR'
        data-cs='e81f22e1c15d42cd5b00e2322f5f36290de85f40'
        data-r='blog-zhenyuan0502-gitalk'
        data-o='zhenyuan0502'
        data-a='zhenyuan0502'
        data-d='true'
    >Comments</div>


            
            

            

        </div>
        <div class="sidebar">
            <div class="box animated fadeInRight">
                <div class="subbox">
                    <div>
                        <img src="/img/qoobee.jpeg" margin-top=1rem height=200 width=200></img>
                        <p>Nguyen Luu<br>(Alfred - Yuan)</p>
                        <span>Think like an artist, develop like an artisan</span>
                        <dl>
                            <dd><a href="https://github.com/zhenyuan0502" target="_blank"><span
                                        class=" iconfont icon-github"></span></a></dd>
                            <dd><a href="https://twitter.com/zhenyuan0502" target="_blank"><span
                                        class=" iconfont icon-twitter"></span></a></dd>
                            <dd><a href="https://stackoverflow.com/users/3789481/" target="_blank"><span
                                        class=" iconfont icon-stack-overflow"></span></a></dd>
                        </dl>
                    </div>
                </div>
                <ul>
                    <li><a href="/">15 <p>Articles</p></a></li>
                    <li><a href="/categories">15 <p>Categories</p></a></li>
                    <li><a href="/tags">36 <p>Tags</p></a></li>
                </ul>
            </div>
            
            
            
            <div class="box sticky animated fadeInRight faster">
                <div id="toc" class="subbox">
                    <div>
                        <h4>Table of Contents</h4>
                        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#The-Art-of-LLM-Prompt-Engineering-for-Translation"><span class="toc-number">1.</span> <span class="toc-text">The Art of LLM Prompt Engineering for Translation</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#9-LLM-Prompt-Design-and-Processing"><span class="toc-number">1.1.</span> <span class="toc-text">9. LLM Prompt Design and Processing</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#The-Anatomy-of-an-Effective-Translation-Prompt"><span class="toc-number">1.1.1.</span> <span class="toc-text">The Anatomy of an Effective Translation Prompt</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Iterative-Refinement-Through-Feedback-Loops"><span class="toc-number">1.1.2.</span> <span class="toc-text">Iterative Refinement Through Feedback Loops</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Model-Selection-and-Parameter-Tuning"><span class="toc-number">1.1.3.</span> <span class="toc-text">Model Selection and Parameter Tuning</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-Validation-and-Quality-Assurance"><span class="toc-number">1.2.</span> <span class="toc-text">10. Validation and Quality Assurance</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Advanced-Validation-Techniques"><span class="toc-number">1.2.1.</span> <span class="toc-text">Advanced Validation Techniques</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Wrapping-up"><span class="toc-number">2.</span> <span class="toc-text">Wrapping up</span></a></li></ol>
                    </div>
                </div>
            </div>
            
            
        </div>
    </div>
</div>

    </div>
</div>
    <div id="back-to-top" class="animated fadeIn faster">
        <div class="flow"></div>
        <span class="percentage animated fadeIn faster">0%</span>
        <span class="iconfont icon-top02 animated fadeIn faster"></span>
    </div>
</body>
<footer>
    <p class="copyright" id="copyright">
        &copy; 2025
        <span class="gradient-text">
            Nguyen Luu
        </span> |
        Powered by <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a>
    </br>
       <span class="gradient-text">nguyenluu.dev's fork v3.0.0</span> from theme
        <span class="gradient-text">
            <a href="https://github.com/TriDiamond/hexo-theme-obsidian" title="Obsidian" target="_blank" rel="noopener">Obsidian</a>
        </span>
        <small><a href="https://github.com/TriDiamond/hexo-theme-obsidian/blob/master/CHANGELOG.md" title="v1.4.5" target="_blank" rel="noopener">v1.4.5</a></small>
    </p>
 
</footer>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script>
  MathJax.Hub.Config({
    "HTML-CSS": {
      preferredFont: "TeX",
      availableFonts: ["STIX", "TeX"],
      linebreaks: {
        automatic: true
      },
      EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
      inlineMath: [
        ["$", "$"],
        ["\\(", "\\)"]
      ],
      processEscapes: true,
      ignoreClass: "tex2jax_ignore|dno",
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      noUndefined: {
        attributes: {
          mathcolor: "red",
          mathbackground: "#FFEEEE",
          mathsize: "90%"
        }
      },
      Macros: {
        href: "{}"
      }
    },
    messageStyle: "none"
  });
</script>
<script>
  function initialMathJax() {
    MathJax.Hub.Queue(function () {
      var all = MathJax.Hub.getAllJax(),
        i;
      // console.log(all);
      for (i = 0; i < all.length; i += 1) {
        console.log(all[i].SourceElement().parentNode)
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  }

  function reprocessMathJax() {
    if (typeof MathJax !== 'undefined') {
      MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
    }
  }
</script>


 
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.min.css">
 
<script src="//cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.min.js"></script>
  
<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
<script src="/js/plugin.js"></script>
<script src="/js/obsidian.js"></script>
<script src="/js/jquery.truncate.js"></script>
<script src="/js/search.js"></script>
 
<script src="//cdnjs.cloudflare.com/ajax/libs/typed.js/2.0.10/typed.min.js"></script>
 
<script src="//cdnjs.cloudflare.com/ajax/libs/blueimp-md5/2.12.0/js/md5.min.js"></script>
 
<script src="//cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/js/social-share.min.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.48.4/codemirror.min.js"></script>
 
<script src="//cdnjs.cloudflare.com/ajax/libs/codemirror/5.48.4/mode/htmlmixed/htmlmixed.min.js"></script>
  
<script src="//cdnjs.cloudflare.com/ajax/libs/codemirror/5.48.4/mode/python/python.min.js"></script>
  
<script src="//cdnjs.cloudflare.com/ajax/libs/codemirror/5.48.4/mode/markdown/markdown.min.js"></script>
   
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3//photoswipe.min.css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3//default-skin/default-skin.min.css">


<script src="//cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3//photoswipe.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3//photoswipe-ui-default.min.js"></script>


<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>
    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">
        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>
        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">
            <div class="pswp__top-bar">
                <!--  Controls are self-explanatory. Order can be changed. -->
                <div class="pswp__counter"></div>
                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
                <button class="pswp__button pswp__button--share" title="Share"></button>
                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>
            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>
            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>
            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>
            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>
        </div>
    </div>
</div>
  
<!-- Highlight.js -->
<!-- 
<link rel="stylesheet" href="/css/dracula.css">
 -->
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js"></script>
<script>
  document.addEventListener('DOMContentLoaded', event => {
    document.querySelectorAll('pre code').forEach(block => {
      hljs.highlightBlock(block);
    });
  });
</script>


<script>
  function initialTyped() {
    var typedTextEl = $('.typed-text');
    if (typedTextEl && typedTextEl.length > 0) {
      var typed = new Typed('.typed-text', {
        strings: ['Think like an artist, develop like an artisan', '艺术家思维去思考问题，工匠创造精神去开发'],
        typeSpeed: 90,
        loop: true,
        loopCount: Infinity,
        backSpeed: 20,
      });
    }
  }

  if ($('.article-header') && $('.article-header').length) {
    $(document).ready(function () {
      initialTyped();
    });
  }
</script>






</html>
